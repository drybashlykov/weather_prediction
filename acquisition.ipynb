{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e43b34-689d-4708-aa62-0fe239bf2605",
   "metadata": {},
   "source": [
    "<h2>Here we scrape</h2> weather underground for historical weather data from the Haifa Airport. They long since eliminated their free API and I heard their pricing now starts at about 800 USD per month, even though I wasn't able to find any mention of their API period. So I scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766d281b-d6b2-4cee-8a39-83fb72a22962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from io import StringIO \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e021002-9919-4812-a848-ee3346bb19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the list of dates\n",
    "\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime.now()\n",
    "\n",
    "date_list = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "             for i in range((end_date - start_date).days + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2109097f-f535-4af9-861d-e7c154611f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Thank you QHarr on stackoverflow https://stackoverflow.com/questions/55306320/scraping-wunderground-without-api-using-python\n",
    "# Updated the expected condition from the original answer to make it a bit quicker and not wait for all tables to load\n",
    "# (which probably included ads)\n",
    "\n",
    "base_url = 'https://www.wunderground.com/history/daily/il/haifa/LLHA/date/'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_pages(date_list, ignore_timeouts=True):\n",
    "    for date in tqdm(date_list, desc=\"Scraping Pages\"):\n",
    "        try:\n",
    "            url = f\"{base_url}{date}\"\n",
    "            driver.get(url)\n",
    "            tables = WebDriverWait(driver,20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"observation-table\")))\n",
    "            for table in tables:\n",
    "                newTable = pd.read_html(StringIO(table.get_attribute('outerHTML')))\n",
    "                if newTable:\n",
    "                    newTable[0].to_csv(f\"saved_tables//{date}.csv\")\n",
    "            time.sleep(1)\n",
    "        except TimeoutException:\n",
    "            if ignore_timeouts:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Failed to scrape \", url)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7014487-d5d4-4df9-8318-298f7129efcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922b737985f44fc1a4b13f306285e593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Pages:   0%|          | 0/2639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrape_pages(date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6aedc2-6c77-4031-96e4-4badd8b47ae2",
   "metadata": {},
   "source": [
    "<h3>Let's see what pages are missing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad825f7e-7869-4d3f-8713-2d5a9d5bb1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool one-liner by pycruft on stackoverflow\n",
    "# https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "\n",
    "path = \"saved_tables\"\n",
    "\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b597d02c-a971-49b8-ab30-bab6e13fd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_dates = [filename[:-4] for filename in files]\n",
    "\n",
    "missing_dates = list(set(date_list) - set(scraped_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe3396b4-fbd6-4af8-b0db-1994d6698b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379d0f22c73042349b08a19544ef05e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Pages:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape  https://www.wunderground.com/history/daily/il/haifa/LLHA/date/2024-06-29\n"
     ]
    }
   ],
   "source": [
    "scrape_pages(missing_dates, ignore_timeouts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1985144-83af-4f78-8948-bf51bf476c07",
   "metadata": {},
   "source": [
    "Some pages are missing because the data is missing on the website, so we check every timeout manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10b9f540-4452-4d63-ae64-e64ae77f2bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fceb8b1a3f9340089a198169b76e7ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping Pages:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrape_pages(missing_dates[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
